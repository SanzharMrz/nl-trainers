{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load('tot_segments_3.jbl').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = data.Topic.apply(lambda row: '2.1. Контактное сопротивление (Абонент)' if row =='2.1. Контактное сопротивление' else row)\n",
    "\n",
    "unique_topics = data['Topic'].unique()\n",
    "val_to_id, id_to_val = {val:i for i, val in enumerate(unique_topics)}, {i:val for i,val in enumerate(unique_topics)}\n",
    "\n",
    "data['binarized_target'] = data.Topic.apply(lambda x: val_to_id.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smurzakhmetov/train_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import (AutoTokenizer, AutoModel)\n",
    "\n",
    "from transformers import (T5ForConditionalGeneration, T5Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
    "\n",
    "paraphrase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "paraphrase_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "paraphrase_model = paraphrase_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_classes, config):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config.get('bert_model'))\n",
    "        self.linear1 = nn.Linear(config.get('hidden_size'), 256)\n",
    "        self.linear2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        sequence_output = self.bert(ids, attention_mask=mask).last_hidden_state\n",
    "        linear1_output = self.linear1(sequence_output[:, 0, :].view(-1,768))\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "        return linear2_output\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataframe, config, is_test=False):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.get('bert_model'), do_lower_case=True)\n",
    "        self.max_len = config.get('max_len')\n",
    "        self.aug_inserter = naw.ContextualWordEmbsAug(\n",
    "            model_path=\"DeepPavlov/distilrubert-base-cased-conversational\", \n",
    "            aug_p=0.2, action=\"insert\")\n",
    "        self.aug_masker = naw.ContextualWordEmbsAug(\n",
    "            model_path=\"blinoff/roberta-base-russian-v0\", aug_p=0.2)\n",
    "        self.aug_translator = naw.back_translation.BackTranslationAug(\n",
    "            from_model_name=\"Helsinki-NLP/opus-mt-ru-en\", to_model_name='Helsinki-NLP/opus-mt-en-ru', device=\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_text = self.dataframe.loc[idx, 'Segment']\n",
    "        randomized_select = random.random()\n",
    "        \n",
    "        if  0 < randomized_select <= 0.25:\n",
    "            augmented_text = self.aug_inserter.augment(original_text)\n",
    "        elif 0.25 < randomized_select <= 0.50:\n",
    "            augmented_text = self.aug_translator.augment(original_text)\n",
    "        elif 0.5 < randomized_select <= 0.75:\n",
    "            augmented_text = self.paraphraser(original_text)\n",
    "        else:\n",
    "            augmented_text = original_text\n",
    "        sentence = \"[CLS] \" + augmented_text + \" [SEP]\"\n",
    "        global inputs\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "            }\n",
    "        else:\n",
    "            targets = torch.tensor(\n",
    "                [self.dataframe.loc[idx, 'binarized_target']])\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "                'targets': targets\n",
    "            }\n",
    "    \n",
    "    def paraphraser(self, text, beams=5, grams=4, do_sample=False):\n",
    "        x = paraphrase_tokenizer(text, return_tensors='pt', padding=True).to(paraphrase_model.device)\n",
    "        max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
    "        out = paraphrase_model.generate(**x, encoder_no_repeat_ngram_size=grams,\n",
    "                             num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
    "        return paraphrase_tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    scaler = GradScaler()\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_dataloader,\n",
    "            valid_dataloader,\n",
    "            device,\n",
    "            config\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_data = train_dataloader\n",
    "        self.valid_data = valid_dataloader\n",
    "        self.loss_fn = self.yield_loss\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "\n",
    "    def yield_loss(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        This is the loss function for this task\n",
    "        \"\"\"\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"\n",
    "        This function trains the model for 1 epoch through all batches\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for _, inputs in enumerate(self.train_data):\n",
    "            self.optimizer.zero_grad()\n",
    "            ids = inputs['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = inputs['mask'].to(self.device, dtype=torch.long)\n",
    "            targets = inputs['targets'].to(self.device)\n",
    "            outputs = self.model(ids=ids, mask=mask)\n",
    "            outputs = nn.functional.log_softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, targets.squeeze(1))\n",
    "            Util.scaler.scale(loss).backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            Util.scaler.step(self.optimizer)\n",
    "            Util.scaler.update()\n",
    "            self.scheduler.step()\n",
    "        res_loss = train_loss / len(self.train_data)\n",
    "        return res_loss\n",
    "\n",
    "    def valid_one_epoch(self):\n",
    "        \"\"\"\n",
    "        This function validates the model for one epoch through all batches of the valid dataset\n",
    "        It also returns the validation Root mean squared error for assesing model performance.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, inputs in enumerate(self.valid_data):\n",
    "                ids = inputs['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = inputs['mask'].to(self.device, dtype=torch.long)\n",
    "                targets = inputs['targets'].to(self.device)\n",
    "                outputs = self.model(ids=ids, mask=mask)\n",
    "                loss = self.loss_fn(\n",
    "                    nn.functional.log_softmax(outputs, dim=1), targets.squeeze(1))\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                outputs = np.argmax(outputs, axis=1)\n",
    "                all_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                all_predictions.extend(outputs.tolist())\n",
    "                test_loss += loss.item()\n",
    "        val_metrics = accuracy_score(all_targets, all_predictions) * 100\n",
    "        res_loss = test_loss / len(self.valid_data)\n",
    "\n",
    "        return val_metrics, res_loss\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def yield_optimizer(model, config):\n",
    "    \"\"\"\n",
    "    Returns optimizer for specific parameters\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    return AdamW(optimizer_parameters, lr=config.get('lr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'train':{\n",
    "      'nb_epochs': 8,\n",
    "      'lr': 0.00003,\n",
    "      'max_len': 100,\n",
    "      'hidden_size': 768,\n",
    "      'train_bs': 32,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased',\n",
    "      'experiment_name': 'intent_classification'  ,\n",
    "      'file_name': 'data/train_data_augmented/data.csv',\n",
    "      'freeze_ratio': 0,\n",
    "      'cuda': 'cuda:0',\n",
    "      'text_embeddings': {'bert':1, 'tfidf':0, 'fasttext':0},\n",
    "      'simple_models': {'naive_bayes':0,'logreg':1,'svm':0,'sgd':0},\n",
    "      'model_name': 'check_dataset_rubert_tiny',\n",
    "      'last_layer': 'logreg'},\n",
    "\n",
    "    'inference': {\n",
    "      'val_size': 0.2,\n",
    "      'valid_bs': 32,\n",
    "      'max_len': 100,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased'},\n",
    "\n",
    "    'evaluation': {\n",
    "      'max_len': 100,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased',\n",
    "      'freeze_ratio': 0,\n",
    "      'hidden_size': 768,\n",
    "      'eval_set_path': '/hdd/conda_kaldi/dvc_storage/clf_intent/val_set/gold_test_set.csv',\n",
    "      'general_set_path': '/hdd/conda_kaldi/dvc_storage/clf_intent/general_set/nonmatched.csv',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(data,\n",
    "                                      test_size=0.2,\n",
    "                                      stratify=data['binarized_target'], random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "train_set = BERTDataset(\n",
    "    dataframe=train_df,\n",
    "    config=Config.get('train')\n",
    ")\n",
    "\n",
    "valid_set = BERTDataset(\n",
    "    dataframe=valid_df,\n",
    "    config=Config.get('inference')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=Config.get('train')['train_bs'],\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "valid = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=Config.get('inference')['valid_bs'],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CustomBERTModel(num_classes=len(\n",
    "    val_to_id), config=Config.get('train')).to(device)\n",
    "\n",
    "nb_train_steps = (\n",
    "    train_df.shape[0] / Config.get('train')['train_bs'] * Config.get('train')['nb_epochs'])\n",
    "\n",
    "optimizer = yield_optimizer(model, Config.get('train'))\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=nb_train_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, optimizer, scheduler, train,\n",
    "                valid, device, Config.get('train'))\n",
    "\n",
    "best_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- EPOCH: 1 ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_set, valid_loss_set = [], []\n",
    "\n",
    "for epoch in tqdm(range(1, Config['train'].get('nb_epochs') + 1)):\n",
    "    print(f\"\\n{'--'*5} EPOCH: {epoch} {'--'*5}\\n\")\n",
    "\n",
    "    # Train for 1 epoch\n",
    "    epoch_loss = trainer.train_one_epoch()\n",
    "    train_loss_set.append(epoch_loss)\n",
    "\n",
    "    # Validate for 1 epoch\n",
    "    val_accuracy, val_loss = trainer.valid_one_epoch()\n",
    "    valid_loss_set.append(val_loss)\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.plot(valid_loss_set)\n",
    "    plt.title(\"Training/Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
