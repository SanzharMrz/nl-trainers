{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load('tot_segments_3.jbl').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic'] = data.Topic.apply(lambda row: '2.1. Контактное сопротивление (Абонент)' if row =='2.1. Контактное сопротивление' else row)\n",
    "\n",
    "unique_topics = data['Topic'].unique()\n",
    "val_to_id, id_to_val = {val:i for i, val in enumerate(unique_topics)}, {i:val for i,val in enumerate(unique_topics)}\n",
    "\n",
    "data['binarized_target'] = data.Topic.apply(lambda x: val_to_id.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smurzakhmetov/train_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import (AutoTokenizer, AutoModel)\n",
    "\n",
    "from transformers import (T5ForConditionalGeneration, T5Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
    "\n",
    "paraphrase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "paraphrase_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "paraphrase_model = paraphrase_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_classes, config):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config.get('bert_model'))\n",
    "        self.linear1 = nn.Linear(config.get('hidden_size'), 256)\n",
    "        self.linear2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        sequence_output = self.bert(ids, attention_mask=mask).last_hidden_state\n",
    "        linear1_output = self.linear1(sequence_output[:, 0, :].view(-1,768))\n",
    "        linear1_output = nn.functional.relu(linear1_output)\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "        return linear2_output\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataframe, config, is_test=False):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.get('bert_model'), do_lower_case=True)\n",
    "        self.max_len = config.get('max_len')\n",
    "        self.aug_inserter = naw.ContextualWordEmbsAug(\n",
    "            model_path=\"DeepPavlov/distilrubert-base-cased-conversational\", \n",
    "            aug_p=0.2, action=\"insert\")\n",
    "        self.aug_masker = naw.ContextualWordEmbsAug(\n",
    "            model_path=\"blinoff/roberta-base-russian-v0\", aug_p=0.2)\n",
    "        self.aug_translator = naw.back_translation.BackTranslationAug(\n",
    "            from_model_name=\"Helsinki-NLP/opus-mt-ru-en\", to_model_name='Helsinki-NLP/opus-mt-en-ru', device=\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_text = self.dataframe.loc[idx, 'Segment']\n",
    "        randomized_select = random.random()\n",
    "        \n",
    "        if  0 < randomized_select <= 0.25:\n",
    "            augmented_text = self.aug_inserter.augment(original_text)\n",
    "        elif 0.25 < randomized_select <= 0.50:\n",
    "            augmented_text = self.aug_translator.augment(original_text)\n",
    "        elif 0.5 < randomized_select <= 0.75:\n",
    "            augmented_text = self.paraphraser(original_text)\n",
    "        else:\n",
    "            augmented_text = original_text\n",
    "        sentence = \"[CLS] \" + augmented_text + \" [SEP]\"\n",
    "        global inputs\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "            }\n",
    "        else:\n",
    "            targets = torch.tensor(\n",
    "                [self.dataframe.loc[idx, 'binarized_target']])\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "                'targets': targets\n",
    "            }\n",
    "    \n",
    "    def paraphraser(self, text, beams=5, grams=4, do_sample=False):\n",
    "        x = paraphrase_tokenizer(text, return_tensors='pt', padding=True).to(paraphrase_model.device)\n",
    "        max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
    "        out = paraphrase_model.generate(**x, encoder_no_repeat_ngram_size=grams,\n",
    "                             num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
    "        return paraphrase_tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    scaler = GradScaler()\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_dataloader,\n",
    "            valid_dataloader,\n",
    "            device,\n",
    "            config\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_data = train_dataloader\n",
    "        self.valid_data = valid_dataloader\n",
    "        self.loss_fn = self.yield_loss\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "\n",
    "    def yield_loss(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        This is the loss function for this task\n",
    "        \"\"\"\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"\n",
    "        This function trains the model for 1 epoch through all batches\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for _, inputs in enumerate(self.train_data):\n",
    "            self.optimizer.zero_grad()\n",
    "            ids = inputs['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = inputs['mask'].to(self.device, dtype=torch.long)\n",
    "            targets = inputs['targets'].to(self.device)\n",
    "            outputs = self.model(ids=ids, mask=mask)\n",
    "            outputs = nn.functional.log_softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, targets.squeeze(1))\n",
    "            Util.scaler.scale(loss).backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            Util.scaler.step(self.optimizer)\n",
    "            Util.scaler.update()\n",
    "            self.scheduler.step()\n",
    "        res_loss = train_loss / len(self.train_data)\n",
    "        return res_loss\n",
    "\n",
    "    def valid_one_epoch(self):\n",
    "        \"\"\"\n",
    "        This function validates the model for one epoch through all batches of the valid dataset\n",
    "        It also returns the validation Root mean squared error for assesing model performance.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, inputs in enumerate(self.valid_data):\n",
    "                ids = inputs['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = inputs['mask'].to(self.device, dtype=torch.long)\n",
    "                targets = inputs['targets'].to(self.device)\n",
    "                outputs = self.model(ids=ids, mask=mask)\n",
    "                loss = criterion(\n",
    "                    nn.functional.log_softmax(outputs, dim=1), targets.squeeze(1))\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                outputs = np.argmax(outputs, axis=1)\n",
    "                all_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                all_predictions.extend(outputs.tolist())\n",
    "                test_loss += loss.item()\n",
    "        val_metrics = accuracy_score(all_targets, all_predictions) * 100\n",
    "        res_loss = test_loss / len(self.valid_data)\n",
    "\n",
    "        return val_metrics, res_loss\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def yield_optimizer(model, config):\n",
    "    \"\"\"\n",
    "    Returns optimizer for specific parameters\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    return AdamW(optimizer_parameters, lr=config.get('lr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'train':{\n",
    "      'nb_epochs': 8,\n",
    "      'lr': 0.00003,\n",
    "      'max_len': 100,\n",
    "      'hidden_size': 768,\n",
    "      'train_bs': 32,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased',\n",
    "      'experiment_name': 'intent_classification'  ,\n",
    "      'file_name': 'data/train_data_augmented/data.csv',\n",
    "      'freeze_ratio': 0,\n",
    "      'cuda': 'cuda:0',\n",
    "      'text_embeddings': {'bert':1, 'tfidf':0, 'fasttext':0},\n",
    "      'simple_models': {'naive_bayes':0,'logreg':1,'svm':0,'sgd':0},\n",
    "      'model_name': 'check_dataset_rubert_tiny',\n",
    "      'last_layer': 'logreg'},\n",
    "\n",
    "    'inference': {\n",
    "      'val_size': 0.2,\n",
    "      'valid_bs': 32,\n",
    "      'max_len': 100,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased'},\n",
    "\n",
    "    'evaluation': {\n",
    "      'max_len': 100,\n",
    "      'bert_model': 'DeepPavlov/rubert-base-cased',\n",
    "      'freeze_ratio': 0,\n",
    "      'hidden_size': 768,\n",
    "      'eval_set_path': '/hdd/conda_kaldi/dvc_storage/clf_intent/val_set/gold_test_set.csv',\n",
    "      'general_set_path': '/hdd/conda_kaldi/dvc_storage/clf_intent/general_set/nonmatched.csv',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(data,\n",
    "                                      test_size=0.2,\n",
    "                                      stratify=data['binarized_target'], random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "train_set = BERTDataset(\n",
    "    dataframe=train_df,\n",
    "    config=Config.get('train')\n",
    ")\n",
    "\n",
    "valid_set = BERTDataset(\n",
    "    dataframe=valid_df,\n",
    "    config=Config.get('inference')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=Config.get('train')['train_bs'],\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "valid = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=Config.get('inference')['valid_bs'],\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CustomBERTModel(num_classes=len(\n",
    "    val_to_id), config=Config.get('train')).to(device)\n",
    "\n",
    "nb_train_steps = (\n",
    "    train_df.shape[0] / Config.get('train')['train_bs'] * Config.get('train')['nb_epochs'])\n",
    "\n",
    "optimizer = yield_optimizer(model, Config.get('train'))\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=nb_train_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, optimizer, scheduler, train,\n",
    "                valid, device, Config.get('train'))\n",
    "\n",
    "best_loss = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [06:08<43:01, 368.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- EPOCH: 2 ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_set, valid_loss_set = [], []\n",
    "\n",
    "for epoch in tqdm(range(1, Config['train'].get('nb_epochs') + 1)):\n",
    "    print(f\"\\n{'--'*5} EPOCH: {epoch} {'--'*5}\\n\")\n",
    "\n",
    "    # Train for 1 epoch\n",
    "    epoch_loss = trainer.train_one_epoch()\n",
    "    train_loss_set.append(epoch_loss)\n",
    "\n",
    "    # Validate for 1 epoch\n",
    "    val_accuracy, val_loss = trainer.valid_one_epoch()\n",
    "    valid_loss_set.append(val_loss)\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.plot(valid_loss_set)\n",
    "    plt.title(\"Training/Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
